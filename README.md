# Loader for replicating data from Oracle into Impala

The aim of this integration is to capture transactions from Oracle databases (source) and to apply them into Impala database (target). The idea is to replicate the source database in almost real time or in a very short period of time.

On the Oracle side, Oracle Golden Gate should be used with the Flat Files Adapter. This adapter should be configured to generate DSV files which contain the transactional data.

## How it works

When starting, the loader creates the final table (if it does not exist) using the configuration.

The loader checks periodically if there is new data to be transferred. If so, the loader follows several steps:

1. Copies new data to HDFS (without checking the contained data).
2. Creates a temporal (external) table in Impala with the new data located in HDFS.
3. Inserts all data from temporal table into final Impala table.
4. Deletes temporal table.
5. Deletes new local data.

## Create Eclipse project

You may want to continue with the development of this project and you want to use Eclipse (https://eclipse.org/) as development environment. If so, you could use Maven (https://maven.apache.org/) in order to generate the needed files for Eclipse.

As requirement you need Maven installed on your machine and run the command below when you are in the project folder.

```
mvn eclipse:eclipse
```

## Build project

The binaries are not shipped with the project, so they need to be generated.

This project depends of others such as Hadoop and Hive. We recommend the version of these dependencies are the same version that the target cluster has. The versions are configured in pom.xml file included in the project, you will find at the beginning two parameters that can be used for this purpose.

```
<hive.version>0.13.1-cdh5.3.3</hive.version>
<hadoop.version>2.5.0-cdh5.3.3</hadoop.version>
```

In the Cloudera repository you can look at the different versions that can be configured.
For Hadoop: https://repository.cloudera.com/cloudera/cloudera-repos/org/apache/hadoop/hadoop-client/
For Hive: https://repository.cloudera.com/cloudera/cloudera-repos/org/apache/hive/hive-common/

Once you have properly configured the versions, the binaries can be generated with the following command. 

```
mvn package
```

Binaries can be found at "target" folder. Two binaries are generated: one contains all the dependencies of the project (ogg-impala-*-jar-with-dependencies.jar), and the other one (ogg-impala*.jar) only contains the developed code. If you use the jar with dependencies, you need to neither deploy any libraries nor add them to the classpath.  

## Golden Gate configuration

An official user guide can be found in this link: https://docs.oracle.com/goldengate/gg12121/gg-adapter/GADAD/flatfile_config.htm

With the purpose that Impala can read the data files generated by the Flat Files Adapter, it should be configured with the following parameters:

  * Remove colon betwenn date and time in timestamps (goldengate.userexit.datetime.removecolon=true)
  * Delimited separated files (writer.mode=DSV)
  * Should not contain metadata (writer.includebefores=false, writer.includecolnames=false, writer.omitplaceholders=true)
  * Should contain all values (writer.diffsonly=false, writer.omitvalues=false)
  * Text as ASCII (writer.rawchars=false)
  * Without quotes (writer.dsv.quotes.policy=none)
  * Separated data files per table (writer.files.onepertable=true)
  * Null values should be represented by an empty string (writer.dsv.nullindicator.chars=)
  * Default Impala delimiter expressed as hexadecimal (dsvwriter.dsv.fielddelim.code=01) 
  * Without escape character (writer.dsv.fielddelim.escaped.chars=)
  
If we use the path B for configuring the loader, these parameters could be different since it is the user who specifies the format of the input files in the queries.

## Loader configuration

We can follow two paths for configuring the loader. If we want the loader generates automatically the Impala queries that will be used, we must follow the configuration shown in path A. Otherwise, we must specify each query in the configuration as shown in path B.

Some parameters are common to both paths:

  * ogg.definition.file.name: path to definition file (it must be created when configuring Flat Files Adapter) (mandatory).
  * ogg.data.folders: input data folders paths separated by commas (mandatory).
  * ogg.control.file.name: path to control file which is generated in each data folder (default: <SCHEMA>.<TABLE>control).

  * batch.between.sec: in seconds the period of time for checking for new data (default: 30).
  * loader.failure.wait: in case of failure the period of time (in seconds) for trying again (default: 60).
  
  * impala.host: Impala host where queries will be run (default: localhost).
  * impala.staging.table.directory: path into HDFS where new data will be stored temporally (default: ogg/staging/).

### Path A) Infer queries

Each parameter that should be configured is shown below.
  
  * impala.table.schema: new final table schema (defaul: original Oracle schema)
  * impala.table.name: new final table name (defaul: original Oracle name)
  * impala.staging.table.schema:
  * impala.staging.table.name:

#### Column customizations

The inferred information related with the column can be modified. You need to specify only the parameters related with the information that you want to customize. This customization will be applied in the final Impala table.

  * impala.table.columns.customize: original column name of the columns that you want to customize (below parameters: <COLUMN_NAME>).
  
Per specified column in the above parameter, we can set the following parameters.  
  
  * impala.table.column.<COLUMN_NAME>.name: new column name (default: original column name)
  * impala.table.column.<COLUMN_NAME>.datatype: new Impala data type (dafault: corresponding Impala data type)
  * impala.table.column.<COLUMN_NAME>.expression: Impala expression to be used for generating final value. It should return the final data type (default: cast(<COLUMN_NAME> as <CORRESPONDING_DATA_TYPE>)).
  
If you set a new data type, a new expression that generates this new data type must be configured.
  
##### New columns

New columns can be added to the final Impala table using the same parameter for customized columns. A not used name should be used and all the parameters regarding the new columns must be configured (name, data type and expression).

The expression must use original column names of other columns.

##### Partitioning columns

In a similar way that we do with customized columns

  * impala.table.partitioning.columns: names separated by commas of partitioning columns
  * impala.table.partitioning.column.<PART_COLUMN_NAME>.datatype: Impala data type
  * impala.table.partitioning.column.<PART_COLUMN_NAME>.expression: Impala expression which generate the value of the partitioning column (should return the specified data type).

### Path B) Setting queries

Instead of infer all the queries, we can configure them directly with the following parameters. All of them are mandatory.

  * impala.staging.table.query.create: query for creating temporal (external) Impala table. This table must be able to read the generated files generated by Flat Files Adapter (the HDFS directory must be the one specified in impala.staging.table.directory).
  * impala.staging.table.query.drop: query for dropping temporally table.
  * impala.table.query.create: query for creating final Impala table (used if final table does not exist).
  * impala.table.query.insert: query for importing data form temporally table into final table.

## Running it!

The loader has been implemented in Java, so we need to run it using the JVM. A parameters file should be created and point out as first argument (if not specified, it will try to find ./config.properties). 

```
java -cp ogg-impala-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
   ch.cern.impala.ogg.datapump.ImpalaDataLoader \
   (path_to_parameters_file)
```







